# =============================================================================
# General Configuration
# =============================================================================
work_dir: ./work_dir/protocol_1

# =============================================================================
# Model Configuration
# =============================================================================
model_saved_name: ./save_model/protocol_1/model

# Model Weights and Checkpoints
weights_file_extract: ./save_model/protocol_1/model.pt

# =============================================================================
# Dataset Configuration
# =============================================================================
# Training Dataset
train_dataset: Pose_300W_LP
train_data_path: ./data/resize_300W_LP
train_file_name: ./file_list/protocol_1/300W_LP.txt

# Test Dataset 1 (BIWI)
test_dataset1: BIWI
test_data_path1: ./data/resize_BIWI
test_file_name1: ./file_list/protocol_1/BIWI.txt

# Test Dataset 2 (AFLW2000)
test_dataset2: AFLW2000
test_data_path2: ./data/resize_AFLW2000
test_file_name2: ./file_list/protocol_1/AFLW2000.txt

# =============================================================================
# Training Hyperparameters
# =============================================================================
# Optimizer Settings - Optimized for better convergence
# optimizer: AdamW
base_lr: 0.001 # Increased slightly for faster initial learning
weight_decay: 1.e-8 # Reduced to prevent over-regularization

# Learning Rate Schedule - Optimized based on training analysis
step: [50, 90, 130] # Earlier and more frequent LR drops
# Alternative aggressive schedule for faster convergence
# step: [20, 40, 80, 160]
# Conservative schedule for stable training
# step: [40, 80, 160]

# Batch and Epoch Settings
batch_size: 100
test_batch_size: 100
num_epoch: 170
